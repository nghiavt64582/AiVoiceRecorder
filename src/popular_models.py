popular_ai_models = [
    {
        "id": "tiiuae/Falcon3-3B-Base",
        "description": "A 3-billion parameter language model by TII, designed for various NLP tasks."
    },
    {
        "id": "meta-llama/Llama-2-7b-chat-hf",
        "description": "A powerful 7-billion parameter language model by Meta, fine-tuned for conversational AI and chat applications."
    },
    {
        "id": "mistralai/Mistral-7B-Instruct-v0.2",
        "description": "A high-performance 7-billion parameter language model by Mistral AI, optimized for instruction following and chat."
    },
    {
        "id": "Qwen/Qwen-1_8B-Chat-Int4",
        "description": "A compact 1.8-billion parameter version of the Qwen model, designed for efficient chat and instruction tasks."
    },
    {
        "id": "Qwen/Qwen-7B-Chat",
        "description": "A large language model from Alibaba Cloud, designed for general-purpose chat and understanding diverse queries."
    },
    {
        "id": "Qwen/Qwen2-7B-Instruct",
        "description": "An advanced 7-billion parameter language model from Alibaba Cloud, optimized for instruction following and conversational tasks."
    },
    {
        "id": "google/gemma-2b-it",
        "description": "A 2-billion parameter language model by Google, designed for efficient text generation and understanding."
    },
    {
        "id": "Qwen/Qwen2-1.5B-Instruct",
        "description": "A smaller, more efficient version of the Qwen model, optimized for instruction following and suitable for resource-constrained environments."
    },
    {
        "id": "Qwen/Qwen3-4B-Thinking-2507",
        "description": "A 4-billion parameter language model from Alibaba Cloud, designed for advanced reasoning and thinking tasks."
    },
    {
        "id": "microsoft/Phi-3-mini-4k-instruct",
        "number_of_parameters": "1.3B",
        "description": "A small, yet powerful language model by Microsoft, designed for instruction following and capable of handling longer contexts."
    },
    {
        "id": "bert-base-uncased",
        "number_of_parameters": "110M",
        "description": "Google's foundational bidirectional encoder model, widely used for text understanding tasks like classification and sentiment analysis."
    },
    {
        "id": "roberta-base",
        "number_of_parameters": "125M",
        "description": "A robustly optimized BERT approach by Facebook, known for improved training methodology leading to better performance on various NLP tasks."
    },
    {
        "id": "google-t5/t5-base",
        "number_of_parameters": "220M",
        "description": "Google's 'Text-to-Text Transfer Transformer' that frames all NLP problems as a text-to-text task, useful for translation, summarization, and more."
    },
    {
        "id": "facebook/bart-large-cnn",
        "number_of_parameters": "406M",
        "description": "A powerful sequence-to-sequence model by Facebook AI, particularly strong in abstractive summarization tasks for news articles."
    },
    {
        "id": "google/pegasus-cnn_dailymail",
        "description": "Google's model specifically designed for abstractive summarization, pre-trained on large datasets to generate highly coherent summaries."
    },
    {
        "id": "openai/whisper-base",
        "description": "OpenAI's robust speech-to-text model, capable of transcribing audio in multiple languages and even translating them into English."
    }
]