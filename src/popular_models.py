popular_ai_models = [
    {
        "id": "meta-llama/Llama-2-7b-chat-hf",
        "description": "A powerful 7-billion parameter language model by Meta, fine-tuned for conversational AI and chat applications."
    },
    {
        "id": "mistralai/Mistral-7B-Instruct-v0.2",
        "description": "A high-performance 7-billion parameter language model by Mistral AI, optimized for instruction following and chat."
    },
    {
        "id": "HuggingFaceH4/zephyr-7b-beta",
        "description": "A fine-tuned version of Mistral 7B, optimized for helpfulness and harmlessness, often used in conversational agents."
    },
    {
        "id": "Qwen/Qwen-7B-Chat",
        "description": "A large language model from Alibaba Cloud, designed for general-purpose chat and understanding diverse queries."
    },
    {
        "id": "google/gemma-2b-it",
        "description": "A 2-billion parameter language model by Google, designed for efficient text generation and understanding."
    },
    {
        "id": "Qwen/Qwen2-1.5B-Instruct",
        "description": "A smaller, more efficient version of the Qwen model, optimized for instruction following and suitable for resource-constrained environments."
    },
    {
        "id": "Qwen/Qwen3-4B-Thinking-2507",
        "description": "A 4-billion parameter language model from Alibaba Cloud, designed for advanced reasoning and thinking tasks."
    },
    {
        "id": "microsoft/Phi-3-mini-4k-instruct",
        "description": "A small, yet powerful language model by Microsoft, designed for instruction following and capable of handling longer contexts."
    },
    {
        "id": "bert-base-uncased",
        "description": "Google's foundational bidirectional encoder model, widely used for text understanding tasks like classification and sentiment analysis."
    },
    {
        "id": "roberta-base",
        "description": "A robustly optimized BERT approach by Facebook, known for improved training methodology leading to better performance on various NLP tasks."
    },
    {
        "id": "distilbert-base-uncased",
        "description": "A distilled version of BERT, offering similar performance with significantly fewer parameters and faster inference, ideal for efficiency."
    },
    {
        "id": "microsoft/deberta-v3-base",
        "description": "Microsoft's advanced BERT-like model, which enhances the attention mechanism and uses a disentangled attention strategy for better performance."
    },
    {
        "id": "google-t5/t5-base",
        "description": "Google's 'Text-to-Text Transfer Transformer' that frames all NLP problems as a text-to-text task, useful for translation, summarization, and more."
    },
    {
        "id": "facebook/bart-large-cnn",
        "description": "A powerful sequence-to-sequence model by Facebook AI, particularly strong in abstractive summarization tasks for news articles."
    },
    {
        "id": "google/pegasus-cnn_dailymail",
        "description": "Google's model specifically designed for abstractive summarization, pre-trained on large datasets to generate highly coherent summaries."
    },
    {
        "id": "openai/whisper-base",
        "description": "OpenAI's robust speech-to-text model, capable of transcribing audio in multiple languages and even translating them into English."
    }
]